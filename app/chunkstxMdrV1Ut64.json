[
    {
        "chunk_id": "chunk_1",
        "text": "Hey there. How's it going everybody? In this video, we're gonna be learning how we can group and aggregate our data. Now, if you don't know what grouping and aggregating really entails, then I'd really recommend sticking around for this video, because basically this is what most people think of when they think of actually analyzing data in a meaningful sense. So this will be the first video where we actually get some statistics back on our datasets and aren't just modifying our DataFrames in different ways. So for example, maybe you want to know what the average salary for a developer is, or maybe you wanna know, how many people from each country knows Python or another programming language. So what we're gonna learn here is going to allow us to answer those types of questions. Now I would like to mention that we do have a sponsor for this series of videos, and that is Brilliant. So I really wanna thank Brilliant for sponsoring this series, and it would be great if you all could check them out using the link in the description section below and support the sponsors. And I'll talk more about their services in just a bit. So with that said, let's go ahead and get started. Okay. So before we start doing some more advanced data analysis, let's start off slow and build up to the more advanced stuff, so that all of this makes sense along the way. So I have my developer survey data open here that we've been using throughout this series. And as usual, if you'd like to follow along, then I have links to this code and the data in the description section below. So let's look at some basic aggregations. So if you don't know what aggregation",
        "start_time": 0.0,
        "end_time": 80.545
    },
    {
        "chunk_id": "chunk_2",
        "text": "developer survey data open here that we've been using throughout this series. And as usual, if you'd like to follow along, then I have links to this code and the data in the description section below. So let's look at some basic aggregations. So if you don't know what aggregation means, basically it means that we're going to be combining multiple pieces of data into a single result. So for example, if you've ever used a mean, median, or mode in mathematics, these are aggregate functions because they take multiple values and give you either the mean, median, or mode of those results. So if we wanted to run some analysis on our developer survey here, one question we might ask is, okay, what is a typical salary for developers who answered this survey? So that might be some good information to have if you're looking for a job and want to get an idea of what these salaries look like at the moment. So to do this, we can grab the median salaries of our data frame. So first, let's look at these salaries. So our salary column within this data frame here of all these survey results is called converted comp, and that is converted to US dollars. It's actually further over here in the survey. It is, about right here. So I'm gonna copy that. Now first, let's just look at this column. So as we've seen before, we can just access the column, just like we're accessing a key of a dictionary, and I'm gonna grab the first let's get the first 15 salaries or so. So I'm gonna look at the head of the of the, results here. And these are salaries here that developers put down for this survey. And these NAN values here",
        "start_time": 80.045,
        "end_time": 169.74
    },
    {
        "chunk_id": "chunk_3",
        "text": "accessing a key of a dictionary, and I'm gonna grab the first let's get the first 15 salaries or so. So I'm gonna look at the head of the of the, results here. And these are salaries here that developers put down for this survey. And these NAN values here just mean not a number. In this context, it means that they just skipped that question in the survey. Okay. So we can see the median salary for this survey just by running the median method on this series. So to do this, I'm gonna go ahead and copy what I have here. And now instead of looking at the head, I can just run median on that series. So if I run this, then we can see that the median salary for this survey was around $57,000. So that takes all of the salary responses from our survey, from this series here, and it gives us the median value of all of those and ignores the NaN values. Now this probably doesn't give us as much information as we'd really like to have. So for example, different comp, countries pay different amounts, since there are different costs of living and things like that, so it would be nice if we could look at the median salary broken down by country, and we'll look at that here in a second when we learn about grouping data. But first, I want to cover a few more basic concepts before we move on to grouping. So So one thing that I'd like to look at is running these aggregate functions on our entire DataFrame. So let's see what we get if we just run this median, function that we just ran on our entire DataFrame instead of just this single series. So",
        "start_time": 169.5,
        "end_time": 249.655
    },
    {
        "chunk_id": "chunk_4",
        "text": "on to grouping. So So one thing that I'd like to look at is running these aggregate functions on our entire DataFrame. So let's see what we get if we just run this median, function that we just ran on our entire DataFrame instead of just this single series. So here, I'm just going to say df. Median. So we're no longer accessing just a single column. So if I run this, then it might take a second to spin up here. So when I do this, it's going to look through our DataFrame and find the columns that contain numerical values where it can grab a median value. And some of these, might not make sense, to use with the median, but others might be pretty useful to us. So for example, we can see that the median age down here at the bottom, for this survey was 29 years old, and the median number of work hours per week, that was 40, which is pretty standard, so that makes sense. Now if you want to get a broad overview of your data and a statistical overview, we can use the describe method on our DataFrame instead. So if I instead run describe instead of median and I run this, then this is gonna give us a broad overview of some different stats. So if we look at the converted comp column here, then we can see a few different stats about this column. So it gives us the count, it gives us the mean, it gives us the standard deviation, the minimum, and then it also gives us the 25, 50, and 75%, quantiles here. Now this 50% marker is just the median value, by the way. And just like we saw before when we looked this, median",
        "start_time": 249.155,
        "end_time": 337.06
    },
    {
        "chunk_id": "chunk_5",
        "text": "count, it gives us the mean, it gives us the standard deviation, the minimum, and then it also gives us the 25, 50, and 75%, quantiles here. Now this 50% marker is just the median value, by the way. And just like we saw before when we looked this, median value up specifically, this is around 57,000. Now this is in scientific notation here, so it looks a little bit different. Basically, this means that we just need to move 4 spots over from the decimal point. So 1, 2, 3, 4, so that would be 57,000 there. So this describe method gives us a bunch of these aggregates in one place, if we just wanna get a quick overview of our data. Now if you're wondering why I wanted to look at the median of our salaries instead of the mean, which is the average, basically it's because the mean is affected too heavily by out outliers. It's not really a good metric to use because a few outliers can affect the average very heavily. We can see that the mean salary up here, if I highlight this right here, if we were to count this up, then that's actually about $127,000 on average, but that gives us an unrealistic expectation of what a typical developer salary is, because the largest salaries in our dataset are just pulling up that average so heavily. So in cases like that, you definitely wanna use the mean instead. I think that's a better representation. Or I'm sorry, you're gonna want to use the median instead because that's a better representation. Now if we only wanted to get this overview for a single column, then we could just run this describe method on a single column as well and get those results for",
        "start_time": 336.66,
        "end_time": 419.505
    },
    {
        "chunk_id": "chunk_6",
        "text": "better representation. Or I'm sorry, you're gonna want to use the median instead because that's a better representation. Now if we only wanted to get this overview for a single column, then we could just run this describe method on a single column as well and get those results for that specific series. Now you might be wondering what that count value is listed at the top of these described results. Now the count value is the number of non NA rows, which basically means that it counts the non missing rows. So in the context of this survey, a missing row just means that the respondent didn't answer that a specific question. So if I look at the count for the converted comp column so I'm gonna go up here and grab this, and instead of grabbing the median, I'm just gonna grab the count, we can see here that only about 55 to 65 or 55 to 56000 people answered that question. Now I think there are about 89,000 rows for this data, so that means that there are about 30,000 people or so who didn't answer the salary question on this survey. Now, I sometimes see the mistake that some people think that the count function will count up the individual values in a specific row and report how many of those values were in the column. But if that's what you're trying to do, then that's what we would use the value counts function for. Now in case that doesn't quite make sense, let's look at an example, to see what this looks like. So for example, we had the question on the survey that asked each person whether they coded in their free time as a hobby. So to see all of these responses for",
        "start_time": 419.26498,
        "end_time": 502.76498
    },
    {
        "chunk_id": "chunk_7",
        "text": "in case that doesn't quite make sense, let's look at an example, to see what this looks like. So for example, we had the question on the survey that asked each person whether they coded in their free time as a hobby. So to see all of these responses for that question, we can look at the hobbyist column. So I'll just access that hobbyist column here and run that, and we can see that we get a series returned here, And these are just a bunch of yes or no questions. So it was just a yes or no question that each person answered. So you might get the survey results back, and you might think to yourself, okay. Well, I can see the responses here in the survey, but I just wanna know how many people answered yes and how many people answered no. So how would we do that? Well, we can get that information with the value counts function. So if I just look at the value counts, and that is value underscore counts, if we run that method on that series, then that is going to give us a breakdown of how many people answered yes and how many people answered no as to whether or not they code as a hobby. So I use the value counts all the time when exploring data, and we can find out some interesting things from our survey by using this on some different fields. So for example, there is a question on this survey that asks each person what social media platform they use the most. So if you're building an app or a website and want to keep track of the most popular social media sites, then you might be interested in what the most popular",
        "start_time": 502.605,
        "end_time": 580.56
    },
    {
        "chunk_id": "chunk_8",
        "text": "there is a question on this survey that asks each person what social media platform they use the most. So if you're building an app or a website and want to keep track of the most popular social media sites, then you might be interested in what the most popular answers to that question are. So to view these results, we can access the social media column of the survey. So let me do that. And before I run value counts on this, let me just show you what, this column looks like. So this column was called social media. So I'm gonna run this, and we can see that respondent number 1 said that they use Twitter more than any other social media. This person used Instagram, Reddit, Reddit, Facebook, YouTube, and so on. Now, I've pointed this out in previous videos so far, but if you've forgotten or if this is your first video that you've watched in this series, then at the top of my notebook here, I've also loaded in a schema data frame, right here. And this DataFrame tells us the exact question that was asked on the survey for each of these column names. So for example, if we wanna see the exact question that was asked for this social media column, then I can just access that schema data frame and do a dot loc because the indexes are going to be the column names, and then we can just search for social media. And if I run that, then we can see that the question that they asked on the survey specifically was, what social media site do you use the most? Okay. So we can see that we get a few different, responses here, but which of these are the most",
        "start_time": 580.16,
        "end_time": 665.45
    },
    {
        "chunk_id": "chunk_9",
        "text": "media. And if I run that, then we can see that the question that they asked on the survey specifically was, what social media site do you use the most? Okay. So we can see that we get a few different, responses here, but which of these are the most popular? So to find that out, let's look at the value counts of this series to see what the most popular social media sites are overall, for these developers. So I'm going to run this, and then I'm going to run that value counts function here. And now we can see here at the top, that Reddit was the most popular with about 14,000 people, and then we have YouTube, WhatsApp, Facebook, Twitter, Instagram. I don't use social media was one of the answers. Now we also have, some foreign social networks here. So I've never heard of these, but, I believe these are Chinese characters, so this is probably a, a Chinese social media site. I don't know, really Russian writing, but I would assume that this is Russian writing here, so this is probably a Russian social media site. So it's kind of interesting seeing all of these different answers from around the world. Now one more quick tip. If we wanna see these broken down by percentage instead of raw numbers, then we can pass in the normalize argument to the value counts function and set that equal to true. So let me show you what this looks like. So I can say normalize equals true. And now we're gonna get these broken down by percentage. So 17% of the people said that they use Reddit, 16 said YouTube, about 16 said WhatsApp, and so on. Okay. So we can see that we have some social media",
        "start_time": 665.13,
        "end_time": 752.84
    },
    {
        "chunk_id": "chunk_10",
        "text": "looks like. So I can say normalize equals true. And now we're gonna get these broken down by percentage. So 17% of the people said that they use Reddit, 16 said YouTube, about 16 said WhatsApp, and so on. Okay. So we can see that we have some social media sites here from some other countries. So obviously, this is most likely a regional thing. My guess would be that the popularity of these social media platforms, varies a lot based on what country you're in. So how would we break up these results so that we can see the most popular social media sites for each country? Now in order to do this, we're going to have to learn about grouping our data. So again, this is a topic that can be a little confusing when you first see it, so let me start off slow so that we can see exactly what's going on here. So first of all, if we want to see specific results based on the country or based on some other column, then we are going to have to group on that specific column. And we have the group by function for this. So what actually does it mean to say that we are going to use the groupby function? So in the Pandas documentation, it says that a groupby operation involves, some combination of splitting the object, applying a function, and combining the results. So I'm gonna try to walk through each of those processes one at a time so that we can see exactly how this works. So again, in the Pandas documentation, it says that a group by operation involves some combination of splitting up our object, applying a function, and then combining those results. So let's do each of those.",
        "start_time": 752.52,
        "end_time": 833.625
    },
    {
        "chunk_id": "chunk_11",
        "text": "those processes one at a time so that we can see exactly how this works. So again, in the Pandas documentation, it says that a group by operation involves some combination of splitting up our object, applying a function, and then combining those results. So let's do each of those. Now first, just for a reference, let's display the value counts for each country so that we can see the countries that have the most results for this particular survey. So to do this, we can just access the country column, and if I run this, we can see that, this gives us the country that each respondent said that they were from. And if we look at the value counts for this, then this is going to tally up all of the unique responses. So we can see that, the majority of this survey was answered by developers in the United States, and, in second was India, then Germany, United Kingdom, Canada, and so on. Okay. So now let's look at how to use the group by function on our country column. So first we're going to split the object, and then we're going to apply a function, and then it will combine those results. So first, let's look at splitting the object. Now, in this case, we want to group all of the results by country. So to do this, we can simply say, df.groupby, and then we will pass in, this is going to be a list of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with",
        "start_time": 833.125,
        "end_time": 919.065
    },
    {
        "chunk_id": "chunk_12",
        "text": "of columns that we want to group on. And I'm just gonna pass in a single column here for country. So if I run this, then what we get back here is this data frame group by object. So what is this object, and what exactly can we do with this? So first, let's explain a bit what this is. So, this object contains a bunch of groups, and to better understand what this is, let's take a look at an individual group, that this DataFrame has. Now, before we do that, I am going to set this as a variable so that we can reuse this, and not have to retype our code over and over, and also it will be easier to read. So I am going to call this country group, and I'm just going to set this equal to this df.groupby. And now, instead of typing this every time, we can just reference this country group variable here. So now let's take a look at one of these groups. So since we grouped our rows by country, then we can grab a specific group by country name. So I'll grab the group for the United States. So to do this, we can say country group dot git_group, and then pass in the name of the group. In this case, I wanna get the group for United States. So if I run this cell, whoops, and this is telling me that country group is not defined, and it's because I didn't rerun this cell up here after I set that variable. So if I run this and grab the group for the United States, then we can see that we get a data frame returned here with some survey results. So, this doesn't look like anything special",
        "start_time": 918.985,
        "end_time": 1004.075
    },
    {
        "chunk_id": "chunk_13",
        "text": "it's because I didn't rerun this cell up here after I set that variable. So if I run this and grab the group for the United States, then we can see that we get a data frame returned here with some survey results. So, this doesn't look like anything special yet, but if I look at the country name for each of these survey results, the country is listed right here. Then we can see that all of these responses are from people who said that they were from the United States. And if I look at the group for India, so if I instead change United States to India here and grab that group, if we look at the country here, then these are all the survey results for people who said that they were from India. So that's what our data frame group by object that we saw before consists of. It has broken up all of the different responses into groups by country name. So this would be similar to running a filter on our original data frame. So I should be able to get these same results for a single country, just by doing what we've seen in previous videos and creating a filter. So I could say, okay, I want to grab, I want our filter to be equal to, anytime the country is equal to the United States, and then I can apply this to our DataFrame by saying, okay, df dot loc, and give me all the results that match that filter. If I run this cell, then we can see over here in the country column, that all of these, results are respondents from the United States. So if we're just looking to get information on a single country, then it's very",
        "start_time": 1003.595,
        "end_time": 1090.5199
    },
    {
        "chunk_id": "chunk_14",
        "text": "give me all the results that match that filter. If I run this cell, then we can see over here in the country column, that all of these, results are respondents from the United States. So if we're just looking to get information on a single country, then it's very similar to just creating a filter like we did here. But instead of just grabbing, the results for 1 country, groupby instead splits all of these responses up by country name. So now that we have all of those split up and grouped by country name, now we can apply a function and bring those results back together. So what kind of function would we like to apply? Well, like I mentioned before, maybe we want to see the most popular social media sites broken down by country. Now if you just wanted to get the most popular social media sites, by the United States or by India, then we've already seen how we can do this. So right here, I have some filtered results down, to where we have the responses for the United States. So we can just do what we did before where we ran the value counts method on the social media column. So I could just say here, at the end, I could access that social media column of that filtered DataFrame, and then I could just run value counts here. So if I run this, then we can see that for the United States, we have Reddit, and Twitter, and Facebook, and YouTube as the top 4, social media sites. And if we wanted to look at these, specifically for India, then I could instead change that filter for India and run this, and we can see that WhatsApp came first, and then YouTube,",
        "start_time": 1090.2799,
        "end_time": 1176.63
    },
    {
        "chunk_id": "chunk_15",
        "text": "we have Reddit, and Twitter, and Facebook, and YouTube as the top 4, social media sites. And if we wanted to look at these, specifically for India, then I could instead change that filter for India and run this, and we can see that WhatsApp came first, and then YouTube, then LinkedIn, and then Facebook. So these are the results for one specific country. But if we were to run this on our data frame group by object, then it will give us the results for all of those country groups. So, if it helps you with how you think about this, you can imagine that it is similar to running a filter and then applying a function like we did here with a single country, but when we group these using the groupby function and then apply a function, then it will combine those groups to give us the results for all of those unique countries. So I think this will make sense once we just see this here. So remember, I called our group up here, country group. If we come down here to the bottom, then we can say, okay, for the country group, now I want to, look at the social media column, and I wanna grab the value counts for that column for that entire country group. So if I run this, then what this returns is a series with the most popular social media sites broken down by country. Now, this actually cuts off a little early here, so let me grab a larger chunk of this series to get a better idea of what this looks like. So right here at the end, I'm just gonna say, dot head and look at the top fifty results or so. So if we run this,",
        "start_time": 1176.13,
        "end_time": 1256.65
    },
    {
        "chunk_id": "chunk_16",
        "text": "a little early here, so let me grab a larger chunk of this series to get a better idea of what this looks like. So right here at the end, I'm just gonna say, dot head and look at the top fifty results or so. So if we run this, then we can see here that our first country is Afghanistan, and we can look at the most popular social media for that, and then go down the list. Albania, Algeria, Argentina, and so on. Now, this is actually returning a series, and this series has multiple indexes. It has this country index and this social media index. Now, we haven't discussed multiple indexes in this series yet, but if anyone is curious about how this works, then maybe just leave a comment in the description section below, and maybe we can cover that topic in a future video. But, the country is the first index, and we can grab these just like we would with any other series. So again, if I wanted to grab those most popular social media sites, for India, for example, then I could just come up here, and with that returned series actually, let's take a look at this again. So here's the index here. I can grab that series just by saying dotloc, and then looking for India. And we can see that those are the same results that we got before. Now, you might be wondering, well, hey, if those are the same results that I got before, then why is this even useful? And it's useful because now we can see this result with any country without running a filter on each individual country in the world. So, for example, if I wanted to see the most popular social media sites,",
        "start_time": 1256.2649,
        "end_time": 1339.87
    },
    {
        "chunk_id": "chunk_17",
        "text": "same results that I got before, then why is this even useful? And it's useful because now we can see this result with any country without running a filter on each individual country in the world. So, for example, if I wanted to see the most popular social media sites, for the United States, then now instead of, you know, changing a filter over and over, I could just, you know, go here and look at the United States index for this return series, and now we can see those results. So I think it's really interesting, being able to play around with your data like this and being able to explore. I really like seeing the different results for different countries, and a lot of these sites I've never heard of. So for example, if we look at the most popular social media sites in China or in Russia, then let me look at China here. So we can see that, yeah, it does look like that was a Chinese, social media site, this WeChat or WeChat, and then we have I'm assuming this is pronounced Weibo maybe. But, yeah, I think that's very interesting. If we wanna look at Russia, then, we can't actually say just Russia. In this survey, Russia was called the Russian Federation. I've made that mistake before where I just type in Russia, and it'll tell you that it cannot find an index with that name. So this is actually Russian Federation, And if we search for that, then we can see I don't know how to pronounce this, but the one that I thought was Russian writing before, it does look like that was in fact Russian. And just remember, if it makes more sense, for you to, look at percentages instead of",
        "start_time": 1339.37,
        "end_time": 1427.675
    },
    {
        "chunk_id": "chunk_18",
        "text": "we search for that, then we can see I don't know how to pronounce this, but the one that I thought was Russian writing before, it does look like that was in fact Russian. And just remember, if it makes more sense, for you to, look at percentages instead of just raw numbers here, then you can always set normalize equal to true, and it will give you percentage results instead of the raw numbers. So we can see that this Russian social media site here, has 30%, or 30% of the people from Russia said that that was their, most popular social network. And if we go back to China, then we can see that this one here at the top, that has 67% of the developers from China said that that was the social media site that they used the most. So I just thought that was really interesting, being able to play around with these numbers and seeing the different results for different countries, and this is the kind of thing that we can do once we have got these skills down within Pandas. And a lot of the times, it's just fun being able to explore your data like this and finding things within your data that you might not have expected. Now bringing this back to what we were discussing at the beginning of the video, we can also use this to run more traditional aggregate functions like mean, median, and things like that. So before, we looked at the median salaries for the entire survey, but now let's break these down by country instead. So just like we looked at the value counts of the social media column, we can look at the median of the salary column, and that salary column is labeled",
        "start_time": 1427.515,
        "end_time": 1505.0
    },
    {
        "chunk_id": "chunk_19",
        "text": "before, we looked at the median salaries for the entire survey, but now let's break these down by country instead. So just like we looked at the value counts of the social media column, we can look at the median of the salary column, and that salary column is labeled converted comp. So to do this, I can just grab our country group here, and we want to look at this converted comp column. And now we need to tell it what aggregate function we wanna see for all these countries, and I want to see the median salaries for all these countries. So if I run this, then we can see that our result here is that it says, okay, here is the median salary in Afghanistan, here it is for Albania, and so on. So now if you wanted to, for example, see the median salary in a place like Germany, then we can just simply come up here, and this is the result that we get here, and these are our indexes. So the index, the indexes are country name. So if I wanna grab a specific country, then I can just use dot loc and type in the country name. So if I run this, then we can see that the median salary here in Germany is about 63,000. Now maybe you're working on some analysis where you want to group your data, but you also want to run multiple aggregate functions on your group. So let's say that we just didn't want to see the median, but we also wanted to see the mean as well. So to do this, we can use the agg method, agg, and pass in all of the aggregate functions that we wanna use. So to do this here, I",
        "start_time": 1504.52,
        "end_time": 1584.415
    },
    {
        "chunk_id": "chunk_20",
        "text": "let's say that we just didn't want to see the median, but we also wanted to see the mean as well. So to do this, we can use the agg method, agg, and pass in all of the aggregate functions that we wanna use. So to do this here, I could just say let me grab, where we ran our median here. Instead of running just the median aggregate function, we're gonna use this ag method here, agg, and now we're gonna pass in a list of the aggregate functions. So let's say that I wanna get the median first, and then I also wanna be able to see the mean. So if we run this, then we can see that we get a data frame with the mean and the median salaries for every country. And, again, just like we did before, if I wanted to narrow this down, by a specific country, then we could easily do that just by grabbing one of these indexes here by country name. So if we wanted to look at the mean and median salaries for Canada, then I could just come up here and say dotloc, and then pass in Canada here. Let me spell that correctly. And now we can see the, median salary and the mean salary for Canada. Now depending on what you're trying to do, you might run into some issues that you didn't quite expect. So for example, let's say that you're trying to figure out how many people in each country know how to use Python. So before we do this to our group, let's first look at how we do this with a single country using the filtering approach that we used earlier. So I'm going to scroll up to where we had that",
        "start_time": 1584.175,
        "end_time": 1666.63
    },
    {
        "chunk_id": "chunk_21",
        "text": "how many people in each country know how to use Python. So before we do this to our group, let's first look at how we do this with a single country using the filtering approach that we used earlier. So I'm going to scroll up to where we had that filter, and I'm gonna copy that and paste that in down here. Then I'm just going to get rid of this value counts section here. So currently, the filter that we have here is we are filtering the countries down to people who said that they were from India. So now, in order to figure out how many people said that they knew Python within this survey, we're going to use the string methods that we've seen in previous videos. And if you don't remember what these look like, then we could do this by doing something like this. We could say, okay. I want, all of the responses, for the people who said that they were from India. And now when I get that result, remember that this result here is just gonna be a filtered version of our data frame, our original data frame. And now we can say, okay, I also want, the language worked with is where they put the different the different languages that they actually use. So if we look at this language worked with column here, then we can see that they list all of the languages that they said that they know, And to see if Python is within, this column here, then I can say dot str and use the string class on that return series and say, okay, we want where the str dot contains Python. So this will return true, for the rows that have Python in the languages",
        "start_time": 1666.3099,
        "end_time": 1753.47
    },
    {
        "chunk_id": "chunk_22",
        "text": "And to see if Python is within, this column here, then I can say dot str and use the string class on that return series and say, okay, we want where the str dot contains Python. So this will return true, for the rows that have Python in the languages worked with, and false for the responses that don't. So if I run this, then this just returns a series of true and false values where it tells us whether the language worked with column for each respondent contained that string of Python. Now, if we want to actually count the number of people who know Python, then we can use the sum function to add all of these up. Now, normally, you might think that sum would only work with numerical data, but sum will also work on booleans. It will count all of the trues as 1 and all the falses as 0. So to find out how many people know Python, then I could simply just do a dot sum here at the end. If I run this, then we can see that around 3,100 people, from India, who answered the survey said that they knew Python as one of the languages that they work with. Now before, when we wanted to run a similar aggregation function on our data frame group by object, we simply took the same approach on our group by object. So So for example, you might think that we could just do something like this. To see all of these, to see how many people knew Python from each country, you might think that we could say, okay, well, I should just be able to do this. I could just say, okay, for this country group, I want to look at this",
        "start_time": 1752.97,
        "end_time": 1836.76
    },
    {
        "chunk_id": "chunk_23",
        "text": "something like this. To see all of these, to see how many people knew Python from each country, you might think that we could say, okay, well, I should just be able to do this. I could just say, okay, for this country group, I want to look at this language worked with column, and then see the strings that contain Python and sum those up. But if I run this here, then we can see that we get an error. Now, like I said in a previous video, sometimes it can be hard to read these Pandas errors and understand exactly what we did wrong, but in this case, it actually gives us a pretty good clue as to what we did wrong. It tells us that we cannot access the attribute string of a series group by object, and then it says try using the apply method instead. So the reason that we get this error here is because this is no longer just a series. Instead, this is a series group by object, and it tells us to instead use the apply method. So, when we run an apply method on a group object like this, we are going to specify a function that we want to be run on every series in this group. And I know that can sound a little bit confusing, so let's actually see what this looks like, and, hopefully, it'll clear this up a bit. So instead of accessing this string class, directly here, I'm instead going to use the apply method. And for anybody following along or who will download this, I'm gonna go ahead and leave this cell with this error here, so that you can run that and reproduce that error, and then I'm gonna do the correct",
        "start_time": 1836.52,
        "end_time": 1919.4249
    },
    {
        "chunk_id": "chunk_24",
        "text": "class, directly here, I'm instead going to use the apply method. And for anybody following along or who will download this, I'm gonna go ahead and leave this cell with this error here, so that you can run that and reproduce that error, and then I'm gonna do the correct way in this cell. So, again, instead of using the string class directly on this series group object, I'm instead going to use the apply method. Let me just cut that out and I'll say dot apply. And now, we can apply a function that we want to run on each series in this group. So if you've seen one of the previous videos, then you'll know that if we just want a nice, quick, easy function, then we can use a Lambda function. You could write another separate function if you wanted to, but here I'm going to use Lambda. So Lambda here is going to be a series. So now we can say, okay. Well, what do we want to return? Alright. Well, I wanna return x, and then since this is a series, we can say x.string.containsPythondot sum. So again, just one more time, we are running the apply method on this series group, and then we are passing in a function that is going to run on each one of these series, and the function that we want or what we want returned from that function is the sum of, any of the values in that series, that contain the string Python, and it's gonna do that for every country since we're using this country group. So if I run this, then we can see here, that we see okay. In Afghanistan, 8 of the respondents said that they know Python, Albania was 23, and so",
        "start_time": 1919.025,
        "end_time": 2014.73
    },
    {
        "chunk_id": "chunk_25",
        "text": "series, that contain the string Python, and it's gonna do that for every country since we're using this country group. So if I run this, then we can see here, that we see okay. In Afghanistan, 8 of the respondents said that they know Python, Albania was 23, and so on. Now seeing these numbers by itself isn't really that big of a help if we're trying to get an understanding of the percentage of people in each country who said that they know Python. Because with these results here, we only see a single number. We'd have to go back and forth and compare, okay, how many people answered their survey from each country, and how many of them use Python, and then we could do a calculation from there to figure out the percentage of people from that country who knew Python. But we don't wanna do that. That is too much to do manually. So we want to figure our way so that we can get Python and Pandas to do this calculation for us. Now, a lot of people have asked me to put together coding problems to practice what we learn in these videos, so you can think of this as practice. So I'll do this here. So can any of you think of a way where we can figure out what percentage of people in each country know how to use Python? If you think that you can figure that out, then you can pause the video here and try to work through this yourself, and it's going to combine a few topics that we've discussed in the series so far in order to do this. But with that said, I'm gonna go ahead and move along with my solution. So, again, if you",
        "start_time": 2014.57,
        "end_time": 2086.02
    },
    {
        "chunk_id": "chunk_26",
        "text": "pause the video here and try to work through this yourself, and it's going to combine a few topics that we've discussed in the series so far in order to do this. But with that said, I'm gonna go ahead and move along with my solution. So, again, if you wanna try to figure that that out on your own, then you can pause the video and try to work that out. And if you did do that, then I hope that you were able to, get something figured out there. But if not, then no worries. Let's go ahead and walk through my solution here, so that you can use this as practice to get better with pandas, so that you can do this type of analysis in the future. So like I said, in order to get the percentage of developers, who know Python for each country, we are going to use a combination of a few different things that we have learned throughout this series so far. Now, there are probably several different ways of answering this question, and if you have a different, way that you answered this question than me, then definitely leave it in the description section below so that people can see different approaches to this. You know, it's absolutely possible that there's a more efficient way than how I'm about to do it here. So if there is, then I'll highlight that so others can see what the best approach is. But here's how I'm gonna do this. So first, I'm gonna grab the total number of respondents from each country. That way, we, know the total number of people from each country who responded to this survey. So I will just call this, country respondents, and I will set this equal",
        "start_time": 2085.86,
        "end_time": 2160.21
    },
    {
        "chunk_id": "chunk_27",
        "text": "here's how I'm gonna do this. So first, I'm gonna grab the total number of respondents from each country. That way, we, know the total number of people from each country who responded to this survey. So I will just call this, country respondents, and I will set this equal to, we want to grab the value counts of the countries here. So if I print out what we get here, we've seen this before. Whoops. And I got an error there because I put county, I meant to put country. So if I look at this, then these are the total number of respondents who said that they were from each country. And again, we saw this earlier in the video. So now I'm gonna grab the total number of people from each country who know Python, and we just did this a second ago right here. But I'll go ahead and do this again and set it as a variable, so that we have all of these steps. So I'm going to grab all of that that we just calculated, and now I'm gonna set this as a variable, and I'm gonna call this, you know, country uses Python, and then I'll set it equal to that. And now let's print out that variable as well. So let me go to the next line here. My computer's kinda giving me some grief. Okay. So these are all the people from each country who said that they know how to use Python. So now we have one variable that is a series that has the total number of people from each country, right here, called country respondents. And then we have another variable that is a series that is the total number of people from each country who know",
        "start_time": 2159.81,
        "end_time": 2246.935
    },
    {
        "chunk_id": "chunk_28",
        "text": "to use Python. So now we have one variable that is a series that has the total number of people from each country, right here, called country respondents. And then we have another variable that is a series that is the total number of people from each country who know Python. So now we need to combine these 2. Now, I'm actually going to use a method here that we haven't discussed in this series yet. So if you got stuck here, then that's completely understandable. I probably should have mentioned this in the video where we appended rows to a data frame, but we can combine more than 1 series together using the Pandas concat function. So let's see what this would look like. So I can say and I'll just call this data frame Python df. And now I'm gonna create a data frame where we concat those 2 series into 1. So I can say pd.concat, and now I'm gonna pass in a list of the series that we want to concatenate. So I want this to be our country respondents and I also want to add in this country uses Python series. And now, we also want to set axis equal to columns because by default, it's going to try to concatenate these on row, but we wanna match up the indexes here so that it concats it, that way instead. So we wanna say axis is equal to columns. And then finally, I'm also gonna put sort is equal to false. Now if you watched a previous video, this isn't absolutely necessary, but if you run it without sort equal to false, then it'll give you a warning saying that in a future version of Pandas, that it'll sort by default or sort by false",
        "start_time": 2246.695,
        "end_time": 2339.815
    },
    {
        "chunk_id": "chunk_29",
        "text": "put sort is equal to false. Now if you watched a previous video, this isn't absolutely necessary, but if you run it without sort equal to false, then it'll give you a warning saying that in a future version of Pandas, that it'll sort by default or sort by false on default, so it's better just to go ahead and specify if you want the, resulting data frame sorted or not. So now let's look at this concatenated data frame here. Okay. So now we have a data frame here where these two series have been concatenated and match up on the same index. So this is a lot more useful because now we can see, okay, there were about 20,000 or 21,000 people, who said that they were from the United States, and about 10,000 people who said that they know Python. So that's definitely a lot better and more useful information. Now, one thing about this new data frame that, we have is some columns that don't really relate to what we're talking about anymore. We can see here that this one is just called country, and this one is called languages worked with. So let's rename these so that they make more sense in the context of what we're actually trying to do. And we saw how to rename columns in a previous video as well. But if you forgot, then you can do this just by grabbing our data frame here, and I'll say, Python df, which is our data frame, dot rename. And now, what do we want to rename? We want to rename the columns, and now I'm going to pass in a dictionary here where the key is the previous value and the value is going to be the updated value. So I",
        "start_time": 2339.415,
        "end_time": 2426.09
    },
    {
        "chunk_id": "chunk_30",
        "text": "which is our data frame, dot rename. And now, what do we want to rename? We want to rename the columns, and now I'm going to pass in a dictionary here where the key is the previous value and the value is going to be the updated value. So I will call this, number of respondents, and then I also wanna change this languages worked with column here, and I wanna change this, to be let's call this NumKnows Python. And if I run this, then we can see that this looks good. We have number of respondents from the United States, and number knows Python from the United States. So that looks good to me. So since it looks good, I'm gonna say in place is equal to true so that it actually modifies our data frame. So if I run that and then look at our data frame one more time, then we can see that it has been updated with those new columns. Now we have the total number of respondents from each country and the number of people who know Python from each country in one data frame. So we have all the information that we need to calculate a percentage. Now all we need to do is create a new column and calculate this. So if you remember, in order to create a new column, we can simply just assign it. So I will call this column, PCT for percentage, knows Python, and now what do we want this to be equal to? Well, if you don't know how to calculate a percentage mathematically, basically what you do is you take the part and then divide that by the whole, and then you multiply that by 100. So our part here is the number of",
        "start_time": 2425.93,
        "end_time": 2523.31
    },
    {
        "chunk_id": "chunk_31",
        "text": "do we want this to be equal to? Well, if you don't know how to calculate a percentage mathematically, basically what you do is you take the part and then divide that by the whole, and then you multiply that by 100. So our part here is the number of people who know Python. So I will grab that and say, python_df and access that series, access that column, and then I wanna divide that by the whole, and the whole are the total number of people from that country. So that is num respondents. And now if we want this to be a whole number percentage, then we can multiply this by 100. Okay. So if I did all of this correctly, and it's very possible I made a mistake, but if I did all this correctly, then we should have a data frame here with the percentage of people who know Python from each country. And now we can work with this just like any other data frame. So let's say that we wanted to sort these results. Now, we learned this in a previous video on how to sort values in a series. So let's say that we want to sort the countries by the largest percentage of respondents who know Python. So to do this, I can just say, Python df.sort underscore values, and if you forget how to do any of this, then you can always go back to our Pandas video where we learned about sorting. So in order to sort by the people who know Python or the percentage, we can say, okay, sort by what did I call this here? Percent knows Python, And then I actually want this to be in ascending order equal to false because I want the largest",
        "start_time": 2523.23,
        "end_time": 2611.41
    },
    {
        "chunk_id": "chunk_32",
        "text": "about sorting. So in order to sort by the people who know Python or the percentage, we can say, okay, sort by what did I call this here? Percent knows Python, And then I actually want this to be in ascending order equal to false because I want the largest percentage of people who know Python at the top. And I was about to put in place equals true first, but let's see what this looks like. Okay. So it looks like that that sort worked and it looks good. So now I'll say in place is equal to true so that it modifies our data frame, And now, we can look at our results here. So we can see here that, some of these are a little misleading here because, you know, a 100% of people from, Sao Tome and Principe know Python, but we only had one person from the country who answered the survey, and he happens to know Python or she. So that is a 100%. So instead, let's look at the head here and grab, see if we can find a country here with a larger number of respondents. So okay. We have 72 people from Uganda, and 47 of them knew Python, so that's 65%. That's pretty good. We have oh, okay. So this is United States. That's not bad either. We have about 21,000 here, about 10,000 new pythons, so that's 48%. So that's in the higher range. That's pretty good. So yeah. I think this is a great way to practice working with pandas, and also it's just fun being able to explore your information in this way. And now that we have a data frame with all this information, then we can also inspect a specific country to see what the",
        "start_time": 2610.93,
        "end_time": 2695.31
    },
    {
        "chunk_id": "chunk_33",
        "text": "I think this is a great way to practice working with pandas, and also it's just fun being able to explore your information in this way. And now that we have a data frame with all this information, then we can also inspect a specific country to see what the percentage of developers are from a specific country who know Python. So for example, instead of looking through what if I wanted to see Japan? Instead of looking through all of these, I could just say, okay, Python df.loc, and since our country names are our, indexes here, then we can just do a dotloc of Japan, and then we can see that we get these statistics for that specific country. Okay. So I know that that may have been a lot to take in and that we covered a lot of ground in this video. We definitely covered some more advanced topics here than we did in previous videos, but I hope this kind of got you a little excited to learn what you can do with Pandas, and the types of problems that we can solve. You know, when you are exploring through your data like this, you're probably gonna make a ton of mistakes along the way. You know, I still make mistakes in pandas all the time. Even in these videos, I've made some mistakes, and I have these scripted out. So it definitely happens, but, you know, each problem that we work through, similar to this, just makes it easier and easier each time to work through additional problems. So if you need to go back and rewatch some of these steps in order to work through these problems like this on your own, then that's completely normal. You know, don't think that just because",
        "start_time": 2695.07,
        "end_time": 2773.055
    },
    {
        "chunk_id": "chunk_34",
        "text": "this, just makes it easier and easier each time to work through additional problems. So if you need to go back and rewatch some of these steps in order to work through these problems like this on your own, then that's completely normal. You know, don't think that just because this may have seemed difficult, that there's something wrong with you. It's definitely normal for this stuff to be a lot of information to take in. And also, like I said before, if you have some other ways of solving the problems that we answered here, then like I said, definitely leave a comment with your solution in the description section below, and I'll take a look at those. And I'll highlight some if they are better than what I did here. Okay. So before we end here, I would like to mention the sponsor of this video, and that is Brilliant. So in this series, we've been learning about pandas and how to analyze data in Python, and Brilliant would be an excellent way to supplement what you learn here with their hands on courses. They have some excellent courses and lessons that do a deep dive on how to think about and analyze data correctly. For data analysis fundamentals, I would really recommend checking out their statistics course, which shows you how to analyze graphs and determine significance in the data. And I would also recommend their machine learning course, which takes data analysis to a new level where you'll learn about the techniques being used that allow machines to make decisions where there's just too many variables for a human to consider. So to support my channel and learn more about Brilliant, you can go to brilliant.org forward slash c m s to sign up for free.",
        "start_time": 2772.575,
        "end_time": 2845.0598
    },
    {
        "chunk_id": "chunk_35",
        "text": "where you'll learn about the techniques being used that allow machines to make decisions where there's just too many variables for a human to consider. So to support my channel and learn more about Brilliant, you can go to brilliant.org forward slash c m s to sign up for free. And also, the first 200 people that go to that link will get 20% off the annual premium subscription, and you can find that link in the description section below. Again, that's brilliant.orgforward/cms. Okay. So I think that's gonna do it for this Pandas video. I hope you feel like you got a good idea for how to use these aggregate functions, and also how we can group our data so that we can explore our data in interesting ways. I would really encourage you to take some time after this video and play around with the data a bit. See if you can answer certain questions that someone might have about this data. So for example, what is the most common education level for people who answered this survey? That's definitely something that we, could answer by what we learned here. So I hope you feel like you got a good introduction to being able to answer those types of questions. Now in the next video, we're going to be learning about how to handle missing data and how to clean up your data. It's very common for data to have missing values, so knowing how to sanitize and clean our data is definitely going to be important. But if anyone has any questions about what we covered in this video, then feel free to ask in the comments section below and I'll do my best to answer those. And if you enjoy these tutorials and would like to",
        "start_time": 2844.5598,
        "end_time": 2915.675
    },
    {
        "chunk_id": "chunk_36",
        "text": "clean our data is definitely going to be important. But if anyone has any questions about what we covered in this video, then feel free to ask in the comments section below and I'll do my best to answer those. And if you enjoy these tutorials and would like to support them, then there are several ways you can do that. The easiest way is to simply like the video and give it a thumbs up. And, also, it's a huge help to share these videos with anyone that you think would find them useful. And if you have the means, you can contribute through Patreon, and there's a link to that page in the description section below. Be sure to subscribe for future videos, and thank you all for watching.",
        "start_time": 2915.5151,
        "end_time": 2932.655
    }
]